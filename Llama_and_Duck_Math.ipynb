{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2afd8d8-6962-4189-b7b4-bce1fba002b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for AlexNet_224_GaussBlur_CPU_all_2025-04-04_22-56-29.csv:\n",
      "Accuracy: 0.8440\n",
      "Precision (macro): 0.8398\n",
      "Recall (macro): 0.8438\n",
      "F1 Score (macro): 0.8414\n",
      "\n",
      "\n",
      "Results for AlexNet_224_GaussBlur_all_2025-04-04_22-21-30.csv:\n",
      "Accuracy: 0.8440\n",
      "Precision (macro): 0.8398\n",
      "Recall (macro): 0.8438\n",
      "F1 Score (macro): 0.8414\n",
      "\n",
      "\n",
      "Results for BasicCNN_77_CPU_all_2025-04-04_22-57-19.csv:\n",
      "Accuracy: 0.7811\n",
      "Precision (macro): 0.7760\n",
      "Recall (macro): 0.7760\n",
      "F1 Score (macro): 0.7760\n",
      "\n",
      "\n",
      "Results for BasicCNN_77_all_2025-04-04_22-20-45.csv:\n",
      "Accuracy: 0.7811\n",
      "Precision (macro): 0.7760\n",
      "Recall (macro): 0.7760\n",
      "F1 Score (macro): 0.7760\n",
      "\n",
      "\n",
      "Results for densenet121_noGaussBlur_CPU_all_2025-04-04_23-15-27.csv:\n",
      "Accuracy: 0.9836\n",
      "Precision (macro): 0.9828\n",
      "Recall (macro): 0.9836\n",
      "F1 Score (macro): 0.9832\n",
      "\n",
      "\n",
      "Results for densenet121_noGaussBlur_cuda_all_2025-04-06_15-28-51.csv:\n",
      "Accuracy: 0.9836\n",
      "Precision (macro): 0.9828\n",
      "Recall (macro): 0.9836\n",
      "F1 Score (macro): 0.9832\n",
      "\n",
      "\n",
      "                                                File  Accuracy  \\\n",
      "0  AlexNet_224_GaussBlur_CPU_all_2025-04-04_22-56...  0.844049   \n",
      "1  AlexNet_224_GaussBlur_all_2025-04-04_22-21-30.csv  0.844049   \n",
      "2        BasicCNN_77_CPU_all_2025-04-04_22-57-19.csv  0.781122   \n",
      "3            BasicCNN_77_all_2025-04-04_22-20-45.csv  0.781122   \n",
      "4  densenet121_noGaussBlur_CPU_all_2025-04-04_23-...  0.983584   \n",
      "5  densenet121_noGaussBlur_cuda_all_2025-04-06_15...  0.983584   \n",
      "\n",
      "   Precision (macro)  Recall (macro)  F1 Score (macro)  \n",
      "0           0.839813        0.843771          0.841370  \n",
      "1           0.839813        0.843771          0.841370  \n",
      "2           0.775956        0.775956          0.775956  \n",
      "3           0.775956        0.775956          0.775956  \n",
      "4           0.982815        0.983622          0.983211  \n",
      "5           0.982815        0.983622          0.983211  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# List of CSV files to process\n",
    "file_list = ['AlexNet_224_GaussBlur_CPU_all_2025-04-04_22-56-29.csv', 'AlexNet_224_GaussBlur_all_2025-04-04_22-21-30.csv', \n",
    "             'BasicCNN_77_CPU_all_2025-04-04_22-57-19.csv', 'BasicCNN_77_all_2025-04-04_22-20-45.csv',\n",
    "            'densenet121_noGaussBlur_CPU_all_2025-04-04_23-15-27.csv', 'densenet121_noGaussBlur_cuda_all_2025-04-06_15-28-51.csv']\n",
    "\n",
    "# Create a results dictionary to store the metrics for each file\n",
    "results = []\n",
    "\n",
    "# Loop through each file\n",
    "for file in file_list:\n",
    "    # Read in the CSV file\n",
    "    df = pd.read_csv(file, sep=',')  # Change sep if needed, e.g., sep='\\t' for tab-separated\n",
    "    \n",
    "    # Make sure the column names are correct (strip whitespace if needed)\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    \n",
    "    # Get true and predicted labels\n",
    "    y_true = df['True Label']\n",
    "    y_pred = df['User Choice']\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Append results for the current file\n",
    "    results.append({\n",
    "        'File': file,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (macro)': precision,\n",
    "        'Recall (macro)': recall,\n",
    "        'F1 Score (macro)': f1\n",
    "    })\n",
    "    \n",
    "    # Print the results for the current file\n",
    "    print(f\"Results for {file}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (macro): {precision:.4f}\")\n",
    "    print(f\"Recall (macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert results into a DataFrame for easy saving\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9ecd6-6c9d-4c33-b5a8-52cf93b43745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
